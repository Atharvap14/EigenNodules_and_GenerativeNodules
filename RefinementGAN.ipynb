{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import random\n",
    "\n",
    "def interpolate_nodule(nodule_tensor, S):\n",
    "    S=abs(int(S.item()))\n",
    "    if(S==0):\n",
    "        S=2\n",
    "    nodule_resized = F.interpolate(nodule_tensor.unsqueeze(0).unsqueeze(1), size=(S, S), mode='bilinear', align_corners=False)\n",
    "    return nodule_resized\n",
    "\n",
    "def add_nodule_to_image(larger_image, nodule, x, y):\n",
    "    x=(x/8).int()\n",
    "    y=(y/8).int()\n",
    "    larger_image=larger_image.unsqueeze(1)\n",
    "    nodule_height, nodule_width = nodule.shape[2:]\n",
    "    # print(nodule_height, nodule_width)\n",
    "    larger_height, larger_width = larger_image.shape[2:]\n",
    "    # print(larger_height, larger_width)\n",
    "\n",
    "\n",
    "    # If the nodule doesn't fit, crop the nodule to fit within the boundaries\n",
    "    if x + nodule_width/2 > larger_width:\n",
    "        x=x-20\n",
    "    if y + nodule_height/2 > larger_height:\n",
    "        y=y-20\n",
    "    if x - nodule_width/2 < 0:\n",
    "       x=20\n",
    "    if y - nodule_height/2 < 0:\n",
    "        y=20\n",
    "    \n",
    "    # Add the nodule to the larger image at location (x, y)\n",
    "    # print(larger_image.shape)\n",
    "    # print(nodule.shape)\n",
    "    # print(x,y)\n",
    "    if(nodule_height%2==0 and nodule_width%2==0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2, x-nodule_width//2:x + nodule_width//2] += nodule\n",
    "    if(nodule_height%2!=0 and nodule_width%2==0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2+1, x-nodule_width//2:x + nodule_width//2] += nodule\n",
    "    if(nodule_height%2==0 and nodule_width%2!=0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2, x-nodule_width//2:x + nodule_width//2+1] += nodule\n",
    "    if(nodule_height%2!=0 and nodule_width%2!=0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2+1, x-nodule_width//2:x + nodule_width//2+1] += nodule\n",
    "    \n",
    "    return larger_image\n",
    "\n",
    "class NoduleGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=10, channels_img=1, features_g=64):\n",
    "        super(NoduleGenerator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: N x z_dim x 1 x 1\n",
    "            self._block1(z_dim, features_g * 16, 3, 1, 1),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 3, 1, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 3, 1, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 3, 1, 1),  # img: 32x32\n",
    "             # Output: N x channels_img x 64 x 64\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(\n",
    "                 features_g * 2, channels_img, 3, 1, 1, bias=False,\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def _block1(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear'),\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "# Refinement Network\n",
    "class RefinementNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefinementNetwork, self).__init__()\n",
    "        # Define the architecture for the refinement network (example)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_image_with_nodule):\n",
    "        x = self.relu(self.conv1(input_image_with_nodule))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.sigmoid(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "# Discriminator (for GAN training)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define the architecture for the discriminator (example)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(256*256*64, 1)  # Adapt input size to match flattened conv output\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        x = self.relu(self.conv1(input_image))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for FC layer\n",
    "        x = self.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# # Define the training process\n",
    "# def train(refinement_network, discriminator,nodule_generator, dataloader, num_epochs, device):\n",
    "#     refinement_network.to(device)\n",
    "#     discriminator.to(device)\n",
    "#     nodule_generator.to(device)\n",
    "\n",
    "#     optimizer_refinement = optim.Adam(refinement_network.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#     optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#     criterion = nn.BCELoss()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for idx, data in enumerate(dataloader):\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 clean,nodule,loc=data\n",
    "#                 clean.to(device)\n",
    "#                 nodule.to(device)\n",
    "#                 loc.to(device)\n",
    "#                 nodules = nodule_generator(torch.randn(data.shape[0], 64)).reshape(-1,64,64).to(device)\n",
    "#                 for i in range(nodules.shape[0]):\n",
    "#                     interpolated_nodule = interpolate_nodule(nodules[i], loc[i,2])\n",
    "#                     clean[i]=add_nodule_to_image(clean[i],interpolated_nodule,loc[i,0],loc[i,1])\n",
    "                \n",
    "#             refinement_network.zero_grad()\n",
    "#             fake_output = refinement_network(clean)\n",
    "#             refinement_loss = torch.mean(torch.abs(fake_output - clean))\n",
    "#             refinement_loss.backward()\n",
    "#             optimizer_refinement.step()\n",
    "\n",
    "#             if(idx%4==0):\n",
    "#                 discriminator.zero_grad()\n",
    "#                 real_output = discriminator(nodule)\n",
    "#                 fake_output = discriminator(refinement_network(clean))\n",
    "#                 real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "#                 fake_loss = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "#                 discriminator_loss = (real_loss + fake_loss) / 2\n",
    "#                 discriminator_loss.backward()\n",
    "#                 optimizer_discriminator.step()\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def gaussian_mask(size, sigma):\n",
    "    \"\"\"\n",
    "    Generates a 2D Gaussian mask.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): Size of the mask (both height and width).\n",
    "    - sigma (float): Standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: 2D Gaussian mask.\n",
    "    \"\"\"\n",
    "    ax = torch.arange(-size // 2 + 1., size // 2 + 1.)\n",
    "    xx, yy = torch.meshgrid([ax, ax])\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "    return kernel \n",
    "\n",
    "def create_gaussian_mask(size=64, sigma=16):\n",
    "    \"\"\"\n",
    "    Creates a 64x64 Gaussian mask.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): Size of the mask (both height and width).\n",
    "    - sigma (float): Standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: 64x64 Gaussian mask.\n",
    "    \"\"\"\n",
    "    mask = gaussian_mask(size, sigma)\n",
    "    return mask\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(refinement_network, discriminator, nodule_generator, dataloader, num_epochs, device):\n",
    "    refinement_network.to(device)\n",
    "    discriminator.to(device)\n",
    "    nodule_generator.to(device)\n",
    "\n",
    "    optimizer_refinement = optim.Adam(refinement_network.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    refinement_losses = []\n",
    "    discriminator_losses = []\n",
    "    gauss_mask=(-1)*(create_gaussian_mask().to(device))\n",
    "    print(\"Oggy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, data in tqdm(enumerate(dataloader)):\n",
    "            with torch.no_grad():\n",
    "                clean, nodule, loc = data\n",
    "                clean = clean.to(device)\n",
    "                nodule = nodule.to(device)\n",
    "                loc = loc.to(device)\n",
    "                nodules = nodule_generator(torch.randn(clean.shape[0], 64,1,1).to(device)).reshape(-1, 64, 64).to(device)\n",
    "                for i in range(nodules.shape[0]):\n",
    "                    # print(\"Mask\",gauss_mask)\n",
    "                    # print(nodules[i])\n",
    "                    \n",
    "                    interpolated_nodule = interpolate_nodule(torch.mul(gauss_mask,nodules[i]), loc[i,0,2])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    clean[i] = add_nodule_to_image(clean[i], interpolated_nodule, loc[i, 0,0], loc[i,0, 1])\n",
    "\n",
    "            refinement_network.zero_grad()\n",
    "            fake_output = refinement_network(clean)\n",
    "            refinement_loss = torch.mean(torch.abs(fake_output - clean))\n",
    "            refinement_loss.backward()\n",
    "            optimizer_refinement.step()\n",
    "\n",
    "            refinement_losses.append(refinement_loss.item())\n",
    "\n",
    "            if (17*epoch+idx+1) % 25 == 0:\n",
    "                discriminator.zero_grad()\n",
    "                real_output = discriminator(nodule)\n",
    "                fake_output1 = discriminator(refinement_network(clean))\n",
    "                real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "                fake_loss = criterion(fake_output1, torch.zeros_like(fake_output1))\n",
    "                discriminator_loss = (real_loss + fake_loss) / 2\n",
    "                discriminator_loss.backward()\n",
    "                optimizer_discriminator.step()\n",
    "\n",
    "                discriminator_losses.append(discriminator_loss.item())\n",
    "\n",
    "            if (17*epoch+idx+1)>25 and idx % 8==0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Iteration [{idx + 1}/{len(dataloader)}]\")\n",
    "                print(f\"Refinement Loss: {refinement_loss.item()}, Discriminator Loss: {discriminator_loss.item()}\")\n",
    "                clean_img=clean[0].cpu().detach().squeeze().numpy()\n",
    "                x=int(loc[0,0,0]/8)\n",
    "                y=int(loc[0,0,1]/8)\n",
    "                s=int(loc[0,0,2]/2)\n",
    "                cv2.rectangle(clean_img,(x-s,y-s),(x+s,y+s),(0,255,0),3)\n",
    "                plt.imshow(clean_img)\n",
    "                # plt.imshow(gauss_mask.cpu().detach().numpy())\n",
    "                # Visualize sample results\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.title('Clean Image')\n",
    "                plt.imshow(clean[0].cpu().detach().squeeze().numpy(), cmap='gray')\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.title('Refined Image')\n",
    "                plt.imshow(fake_output[0].cpu().detach().squeeze().numpy(), cmap='gray')\n",
    "                plt.show()\n",
    "\n",
    "    # Plot the losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(refinement_losses, label='Refinement Loss')\n",
    "    plt.plot(discriminator_losses, label='Discriminator Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_nodules,path_clean,gmm,transform=None):\n",
    "        self.transform = transform\n",
    "        self.gmm=gmm\n",
    "        path_nodules=os.listdir(\"images\\images\")\n",
    "        self.nodule_data=path_nodules\n",
    "        self.clean_data=path_clean\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # TODO: return one item on the index\n",
    "\n",
    "        clean = cv2.imread(f'images/images/{random.choice(self.clean_data)}',cv2.IMREAD_GRAYSCALE)\n",
    "        nodule = cv2.imread(f'images/images/{random.choice(self.nodule_data)}',cv2.IMREAD_GRAYSCALE)\n",
    "        new_samples = gmm.sample(1)\n",
    "        \n",
    "        if self.transform:\n",
    "            clean = self.transform(clean)\n",
    "            nodule = self.transform(nodule)\n",
    "\n",
    "        return clean,nodule,torch.from_numpy(new_samples[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: return the data length\n",
    "        return len(self.nodule_data)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.GaussianBlur(3),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "csv=pd.read_csv(\"jsrt_metadata.csv\") # To learn location of Data\n",
    "data=[csv[csv[\"state\"]!=\"non-nodule\"][\"x\"].tolist(),csv[csv[\"state\"]!=\"non-nodule\"][\"y\"].tolist(),csv[csv[\"state\"]!=\"non-nodule\"][\"size\"].tolist()]\n",
    "data=np.array(data).T\n",
    "num_components = 3  # Number of Gaussian components\n",
    "\n",
    "# Create and train the Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=num_components, covariance_type='full')\n",
    "gmm.fit(data)\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = CustomDataset(csv[csv[\"state\"]!=\"non-nodule\"][\"study_id\"].tolist(),csv[csv[\"state\"]==\"non-nodule\"][\"study_id\"].tolist(),gmm,transform=transform)\n",
    "# Initialize the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=15, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"run\")\n",
    "refinement_network = RefinementNetwork()\n",
    "discriminator = Discriminator()\n",
    "nodulegen=NoduleGenerator(64)\n",
    "nodulegen.load_state_dict(torch.load(\"NoduleGenerator.pt\",map_location=\"cpu\"))\n",
    "# Train the models\n",
    "print(\"Training\")\n",
    "train(refinement_network, discriminator,nodulegen ,dataloader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import random\n",
    "\n",
    "def interpolate_nodule(nodule_tensor, S):\n",
    "    S=abs(int(S.item()))\n",
    "    if(S==0):\n",
    "        S=2\n",
    "    nodule_resized = F.interpolate(nodule_tensor.unsqueeze(0).unsqueeze(1), size=(S, S), mode='bilinear', align_corners=False)\n",
    "    return nodule_resized\n",
    "\n",
    "def add_nodule_to_image(larger_image, nodule, x, y):\n",
    "    x=(x/8).int()\n",
    "    y=(y/8).int()\n",
    "    larger_image=larger_image.unsqueeze(1)\n",
    "    nodule_height, nodule_width = nodule.shape[2:]\n",
    "    # print(nodule_height, nodule_width)\n",
    "    larger_height, larger_width = larger_image.shape[2:]\n",
    "    # print(larger_height, larger_width)\n",
    "\n",
    "\n",
    "    # If the nodule doesn't fit, crop the nodule to fit within the boundaries\n",
    "    if x + nodule_width/2 > larger_width:\n",
    "        x=x-20\n",
    "    if y + nodule_height/2 > larger_height:\n",
    "        y=y-20\n",
    "    if x - nodule_width/2 < 0:\n",
    "       x=20\n",
    "    if y - nodule_height/2 < 0:\n",
    "        y=20\n",
    "    \n",
    "    # Add the nodule to the larger image at location (x, y)\n",
    "    # print(larger_image.shape)\n",
    "    # print(nodule.shape)\n",
    "    # print(x,y)\n",
    "    if(nodule_height%2==0 and nodule_width%2==0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2, x-nodule_width//2:x + nodule_width//2] += nodule\n",
    "    if(nodule_height%2!=0 and nodule_width%2==0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2+1, x-nodule_width//2:x + nodule_width//2] += nodule\n",
    "    if(nodule_height%2==0 and nodule_width%2!=0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2, x-nodule_width//2:x + nodule_width//2+1] += nodule\n",
    "    if(nodule_height%2!=0 and nodule_width%2!=0):\n",
    "\n",
    "        larger_image[:, :, y- nodule_height//2:y + nodule_height//2+1, x-nodule_width//2:x + nodule_width//2+1] += nodule\n",
    "    \n",
    "    return larger_image\n",
    "\n",
    "class NoduleGenerator(nn.Module):\n",
    "    def __init__(self, z_dim=10, channels_img=1, features_g=64):\n",
    "        super(NoduleGenerator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            # Input: N x z_dim x 1 x 1\n",
    "            self._block1(z_dim, features_g * 16, 3, 1, 1),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 3, 1, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 3, 1, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 3, 1, 1),  # img: 32x32\n",
    "             # Output: N x channels_img x 64 x 64\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(\n",
    "                 features_g * 2, channels_img, 3, 1, 1, bias=False,\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def _block1(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Upsample(scale_factor=4, mode='bilinear'),\n",
    "            nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding, bias=False,\n",
    "            ),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "# Refinement Network\n",
    "class RefinementNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RefinementNetwork, self).__init__()\n",
    "        # Define the architecture for the refinement network (example)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=65, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_image_with_nodule):\n",
    "        x = self.relu(self.conv1(input_image_with_nodule))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.sigmoid(self.conv3(torch.cat([x,input_image_with_nodule],dim=1)))\n",
    "        return x\n",
    "\n",
    "# Discriminator (for GAN training)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define the architecture for the discriminator (example)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(256*256*64, 1)  # Adapt input size to match flattened conv output\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_image):\n",
    "        x = self.relu(self.conv1(input_image))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten for FC layer\n",
    "        x = self.sigmoid(self.fc(x))\n",
    "        return x\n",
    "\n",
    "# # Define the training process\n",
    "# def train(refinement_network, discriminator,nodule_generator, dataloader, num_epochs, device):\n",
    "#     refinement_network.to(device)\n",
    "#     discriminator.to(device)\n",
    "#     nodule_generator.to(device)\n",
    "\n",
    "#     optimizer_refinement = optim.Adam(refinement_network.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#     optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "#     criterion = nn.BCELoss()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for idx, data in enumerate(dataloader):\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 clean,nodule,loc=data\n",
    "#                 clean.to(device)\n",
    "#                 nodule.to(device)\n",
    "#                 loc.to(device)\n",
    "#                 nodules = nodule_generator(torch.randn(data.shape[0], 64)).reshape(-1,64,64).to(device)\n",
    "#                 for i in range(nodules.shape[0]):\n",
    "#                     interpolated_nodule = interpolate_nodule(nodules[i], loc[i,2])\n",
    "#                     clean[i]=add_nodule_to_image(clean[i],interpolated_nodule,loc[i,0],loc[i,1])\n",
    "                \n",
    "#             refinement_network.zero_grad()\n",
    "#             fake_output = refinement_network(clean)\n",
    "#             refinement_loss = torch.mean(torch.abs(fake_output - clean))\n",
    "#             refinement_loss.backward()\n",
    "#             optimizer_refinement.step()\n",
    "\n",
    "#             if(idx%4==0):\n",
    "#                 discriminator.zero_grad()\n",
    "#                 real_output = discriminator(nodule)\n",
    "#                 fake_output = discriminator(refinement_network(clean))\n",
    "#                 real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "#                 fake_loss = criterion(fake_output, torch.zeros_like(fake_output))\n",
    "#                 discriminator_loss = (real_loss + fake_loss) / 2\n",
    "#                 discriminator_loss.backward()\n",
    "#                 optimizer_discriminator.step()\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def gaussian_mask(size, sigma):\n",
    "    \"\"\"\n",
    "    Generates a 2D Gaussian mask.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): Size of the mask (both height and width).\n",
    "    - sigma (float): Standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: 2D Gaussian mask.\n",
    "    \"\"\"\n",
    "    ax = torch.arange(-size // 2 + 1., size // 2 + 1.)\n",
    "    xx, yy = torch.meshgrid([ax, ax])\n",
    "    kernel = torch.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
    "    return kernel \n",
    "\n",
    "def create_gaussian_mask(size=64, sigma=16):\n",
    "    \"\"\"\n",
    "    Creates a 64x64 Gaussian mask.\n",
    "\n",
    "    Parameters:\n",
    "    - size (int): Size of the mask (both height and width).\n",
    "    - sigma (float): Standard deviation of the Gaussian distribution.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: 64x64 Gaussian mask.\n",
    "    \"\"\"\n",
    "    mask = gaussian_mask(size, sigma)\n",
    "    return mask\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(refinement_network, discriminator, nodule_generator, dataloader, num_epochs, device):\n",
    "    refinement_network.to(device)\n",
    "    discriminator.to(device)\n",
    "    nodule_generator.to(device)\n",
    "\n",
    "    optimizer_refinement = optim.Adam(refinement_network.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_discriminator = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    refinement_losses = []\n",
    "    discriminator_losses = []\n",
    "    gauss_mask=(-1)*(create_gaussian_mask().to(device))\n",
    "    print(\"Oggy\")\n",
    "    for epoch in range(num_epochs):\n",
    "        for idx, data in tqdm(enumerate(dataloader)):\n",
    "            with torch.no_grad():\n",
    "                clean, nodule, loc = data\n",
    "                clean = clean.to(device)\n",
    "                nodule = nodule.to(device)\n",
    "                loc = loc.to(device)\n",
    "                nodules = nodule_generator(torch.randn(clean.shape[0], 64,1,1).to(device)).reshape(-1, 64, 64).to(device)\n",
    "                for i in range(nodules.shape[0]):\n",
    "                    # print(\"Mask\",gauss_mask)\n",
    "                    # print(nodules[i])\n",
    "                    \n",
    "                    interpolated_nodule = interpolate_nodule(torch.mul(gauss_mask,nodules[i]), loc[i,0,2])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    clean[i] = add_nodule_to_image(clean[i], interpolated_nodule, loc[i, 0,0], loc[i,0, 1])\n",
    "\n",
    "            refinement_network.zero_grad()\n",
    "            fake_output = refinement_network(clean)\n",
    "            refinement_loss = torch.mean(torch.abs(fake_output - clean))\n",
    "            refinement_loss.backward()\n",
    "            optimizer_refinement.step()\n",
    "\n",
    "            refinement_losses.append(refinement_loss.item())\n",
    "\n",
    "            if idx%8 == 0:\n",
    "                discriminator.zero_grad()\n",
    "                real_output = discriminator(nodule)\n",
    "                fake_output1 = discriminator(refinement_network(clean))\n",
    "                real_loss = criterion(real_output, torch.ones_like(real_output))\n",
    "                fake_loss = criterion(fake_output1, torch.zeros_like(fake_output1))\n",
    "                discriminator_loss = (real_loss + fake_loss) / 2\n",
    "                discriminator_loss.backward()\n",
    "                optimizer_discriminator.step()\n",
    "\n",
    "                discriminator_losses.append(discriminator_loss.item())\n",
    "\n",
    "            if  idx % 8==0:\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Iteration [{idx + 1}/{len(dataloader)}]\")\n",
    "                print(f\"Refinement Loss: {refinement_loss.item()}, Discriminator Loss: {discriminator_loss.item()}\")\n",
    "                clean_img=clean[0].cpu().detach().squeeze().numpy()\n",
    "                x=int(loc[0,0,0]/8)\n",
    "                y=int(loc[0,0,1]/8)\n",
    "                s=int(loc[0,0,2]/2)\n",
    "                cv2.rectangle(clean_img,(x-s,y-s),(x+s,y+s),(0,255,0),3)\n",
    "                plt.imshow(clean_img)\n",
    "                # plt.imshow(gauss_mask.cpu().detach().numpy())\n",
    "                # Visualize sample results\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.title('Clean Image')\n",
    "                plt.imshow(clean[0].cpu().detach().squeeze().numpy(), cmap='gray')\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.title('Refined Image')\n",
    "                plt.imshow(fake_output[0].cpu().detach().squeeze().numpy(), cmap='gray')\n",
    "                plt.show()\n",
    "\n",
    "    # Plot the losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(refinement_losses, label='Refinement Loss')\n",
    "    plt.plot(discriminator_losses, label='Discriminator Loss')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, path_nodules,path_clean,gmm,transform=None):\n",
    "        self.transform = transform\n",
    "        self.gmm=gmm\n",
    "        path_nodules=os.listdir(\"images\\images\")\n",
    "        self.nodule_data=path_nodules\n",
    "        self.clean_data=path_clean\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # TODO: return one item on the index\n",
    "\n",
    "        clean = cv2.imread(f'images/images/{random.choice(self.clean_data)}',cv2.IMREAD_GRAYSCALE)\n",
    "        nodule = cv2.imread(f'images/images/{random.choice(self.nodule_data)}',cv2.IMREAD_GRAYSCALE)\n",
    "        new_samples = gmm.sample(1)\n",
    "        \n",
    "        if self.transform:\n",
    "            clean = self.transform(clean)\n",
    "            nodule = self.transform(nodule)\n",
    "\n",
    "        return clean,nodule,torch.from_numpy(new_samples[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO: return the data length\n",
    "        return len(self.nodule_data)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.GaussianBlur(3),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "csv=pd.read_csv(\"jsrt_metadata.csv\")\n",
    "data=[csv[csv[\"state\"]!=\"non-nodule\"][\"x\"].tolist(),csv[csv[\"state\"]!=\"non-nodule\"][\"y\"].tolist(),csv[csv[\"state\"]!=\"non-nodule\"][\"size\"].tolist()]\n",
    "data=np.array(data).T\n",
    "num_components = 3  # Number of Gaussian components\n",
    "\n",
    "# Create and train the Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=num_components, covariance_type='full')\n",
    "gmm.fit(data)\n",
    "\n",
    "# Initialize the dataset\n",
    "dataset = CustomDataset(csv[csv[\"state\"]!=\"non-nodule\"][\"study_id\"].tolist(),csv[csv[\"state\"]==\"non-nodule\"][\"study_id\"].tolist(),gmm,transform=transform)\n",
    "# Initialize the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=15, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"run\")\n",
    "refinement_network = RefinementNetwork()\n",
    "discriminator = Discriminator()\n",
    "nodulegen=NoduleGenerator(64)\n",
    "nodulegen.load_state_dict(torch.load(\"NoduleGenerator.pt\",map_location=\"cpu\"))\n",
    "# Train the models\n",
    "print(\"Training\")\n",
    "train(refinement_network, discriminator,nodulegen ,dataloader, num_epochs=10, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
